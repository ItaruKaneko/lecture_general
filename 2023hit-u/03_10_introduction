# データサイエンスとelsi

## 1. 確率論は比較的新しい

- 微積分、幾何学 ギリシャ時代から存在
- 確率論  16世紀、19世紀(理論化)

確率、統計、データサイエンス、人工知能は、現在もっとも注目されている理論分野の一つだ。しかしこの分野は比較的新しく、数百年の歴史しかない。数百年の歴史は十分に長いと思ったなら、他の数学分野と比較してみるとよい。ピタゴラスの定理は実はピタゴラスが発見したわけではなく、インドで知られていた数学がギリシャに伝わったものである。この定理は数千年前から知られていた、と言われている。現在の幾何学はユークリッド幾何学と呼ばれているようにギリシャ時代にはすでに確立していた。ライプニッツやニュートンにより近代的な数学分野となった微積分学もギリシャ時代にすでに考え方自体はあったのだ。それにくらべると今日の確率論につながる理論体系の創始は16世紀にみられるにすぎない。理論化されたのは19世紀になってからだ。

## 2. なぜ確率論は新しいのだろうか?

- 非確定的 真実は神にしかわからない
- 実験的に証明できない 得られるのは標本

なぜ確率論があたらしいか、といえば非確定的で実証的ではない理論だからだろう。確率論が最初に論じられたのはサイコロの理論であったが、これはようするにギャンブルの理論である。ギャンブルが多くの人の関心時になるためにはそれだけ生活に余力がなければならない。人類が余剰生産力を獲得し、遊興に十分な時間をかけられるようになったのが16世紀以後というわけである。

また実証的ではない。運動方程式は大砲の着地点や天体の運動として、非常に精密に予測をして確実に理論通りの結果がでることを確認できる。一方確率論は対象が確率的だから、ある事象が0.5の確率でおきることを「確認すること」は非常に困難だ。

## 3. 確率、標本、仮説検定

確率、標本、仮説検定という確率論の枠組みを, 以下資料の1～3で再度確認してみよう。

[確率統計の理解](https://github.com/ItaruKaneko/lecture_twuc_ds1/blob/master/md/reference/un_probability.md)


コイントスにより何回連続して表が出てもそれは表が出る確率が高いということにはならないということはよく理解しておく必要がある。
コインの表が連続して出た時にいえることは
- 表が出る確率が1/2である場合にはそれはほぼ怒りえない
- 表が出る確率が1.0であればそれが起こる確率は1である
ということだけだ。

このように、与えられた仮説のものとで標本が得られる確率が求まるだけで標本によって確率が更新されるわけではない。

## 4. 例1: コイントス実験

今年の1月のscientific american にはこのことを再認識させられる実験が行われた。
コインの表が出る確率がいくつか、というコイントス実験だ。
実験により、コイントスで表が出る確率は0.5にはならない、という結果が得られた。

[Scientists Destroy Illusion That Coin Toss Flips Are 50–50](https://www.scientificamerican.com/article/scientists-destroy-illusion-that-coin-toss-flips-are-50-50/)

もちろん、正確に言えば、コイントスの確率が0.5である場合に、実験結果のような標本が得られる可能性は非常に小さい。つまりコイントスの結果が0.5であるという仮説は棄却された、ということであるが、ようするに確率は0.5ではない。コイントスでも確率を自明のこととして仮定することはできないし、サイコロであっても、電子のスピンであってもそうである。いえるのは、これまでの観測結果から、量子的効果によるサイコロであれば

確率0.5

である可能性はきわめて高い。ということだけだ。

このように確率、統計、データサイエンスにおいては、

## 5. 例2: 機能性表示食品

- 効果はどう確認されるか
- デメリットはどう確認されるか
- 有意水準はどの程度か

[機能性表示食品について](https://www.caa.go.jp/policies/policy/food_labeling/foods_with_function_claims/)

効果の有意水準の表示例

[Q10ヨーグルト 臨床試験の結果]
(https://www.kaneka.co.jp/q10yogurt/tyuicare.html)

問題1: 機能性表示食品はどの程度まで大量摂取しても有害でないと確認されているか？
問題2: 効果がまったくない場合、有意水準0.01で効果がある、という結果が得られる確率は?

このように現代生活において提供される情報を解釈するには、確率、統計、データサイエンスの基礎的理解が役立つ。

## 6. 線形回帰

- 線形回帰を爆発的に有用にしたコンピュータ
- 大量のデータから相関性を導くことは容易
- 因果関係と相関関係
- RCT
- 次元の罠

## 6.1 線形代数
線形代数は20世紀後半に重要分野として基礎教育課程にとりいれられた。(1970年代以後).
コンピュータの登場が大量の統計情報の収集とその分析を可能にした。
線形回帰、線形計画法、線形予測はコンピュータ時代がもたらしたデータ処理手法の三種の神器。

線形代数は以下のような体系だった。

### ベクトル
$$
\mathbf{v} = \begin{pmatrix}
v_1 \\
v_2 \\
\vdots \\
v_n
\end{pmatrix}
$$

### 行列
$$
\mathbf{A} = \begin{pmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{m1} & a_{m2} & \cdots & a_{mn}
\end{pmatrix}
$$

### 行列の積
$$
\mathbf{C} = \mathbf{A} \mathbf{B}
$$
$$
\mathbf{C}_{ij} = \sum_{k=1}^{n} a_{ik} b_{kj}
$$

### 行列の逆行列
$$
\mathbf{A}^{-1} \mathbf{A} = \mathbf{I}
$$

### 行列式
$$
\det(\mathbf{A}) = \sum_{\sigma \in S_n} (\text{sgn}(\sigma) \prod_{i=1}^{n} a_{i,\sigma(i)})
$$

### 固有値と固有ベクトル
$$
\mathbf{A} \mathbf{v} = \lambda \mathbf{v}
$$


## 6.2. 線形回帰

### 線形回帰の基本式
$$
y = \beta_0 + \beta_1 x + \epsilon
$$

### 最小二乗法によるパラメータ推定
$$
\hat{\beta} = (X^T X)^{-1} X^T y
$$

### 決定係数 (R^2)
$$
R^2 = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}
$$

## 6.3 線形予測

### 線形予測の基本式
線形予測の基本式は、以下のように表されます。

$$
\hat{y} = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \cdots + \beta_n x_n
$$

ここで、
$$
\hat{y}    は予測される値
 $$
 $$
\beta_0 は切片
$$
$$
\beta_1, \beta_2, \ldots, \beta_n は各説明変数 x_1, x_2, \ldots, x_n に対応
$$

## 6.4 線形計画法

### 線形計画法の基本式

線形計画法の目的は、以下の線形関数を最大化または最小化することです。

$$
\text{maximize or minimize } \mathbf{c}^T \mathbf{x}
$$

ここで、
$$
- \mathbf{c} は係数ベクトル
$$
$$
- \mathbf{x} は変数ベクトル
$$

### 制約条件

線形計画法には、以下のような制約条件が付きます。

$$
\mathbf{A} \mathbf{x} \leq \mathbf{b}
$$

ここで、
$$
- \mathbf{A} は係数行列
$$
$$
- \mathbf{b} は定数ベクトル
$$

### 非負制約

変数は非負であることが求められます。

$$
\mathbf{x} \geq 0
$$

### 例

例えば、以下のような線形計画問題を考えます。

$$
\text{maximize } z = 3x_1 + 2x_2
$$

制約条件は以下の通りです。

$$
\begin{cases}
2x_1 + x_2 \leq 20 \\
4x_1 + 3x_2 \leq 42 \\
x_1 + 2x_2 \leq 18 \\
x_1, x_2 \geq 0
\end{cases}
$$

## 7. LLM の発展を導いたwordvecと共起行列の処理

### Word2Vec
Word2Vecは、単語をベクトル空間に埋め込むための手法です。以下は、Skip-gramモデルの数式です。

#### Skip-gramモデル
Skip-gramモデルは、ある単語 \( w_t \) を与えられたときに、その周辺の単語 \( w_{t-k}, \ldots, w_{t+k} \) を予測するモデルです。

$$
\max \sum_{t=1}^{T} \sum_{-k \leq j \leq k, j \neq 0} \log P(w_{t+j} | w_t)
$$

ここで、\( P(w_{t+j} | w_t) \) は条件付き確率であり、以下のように定義されます。

$$
P(w_{t+j} | w_t) = \frac{\exp(\mathbf{v}_{w_{t+j}} \cdot \mathbf{v}_{w_t})}{\sum_{w=1}^{W} \exp(\mathbf{v}_w \cdot \mathbf{v}_{w_t})}
$$

### 共起行列
共起行列は、単語の共起情報を行列形式で表現したものです。以下は、共起行列 \( \mathbf{C} \) の定義です。

#### 共起行列の定義
共起行列 \( \mathbf{C} \) の要素

